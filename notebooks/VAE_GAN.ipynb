{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gAzgkVIkIXi3"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# High Resolution VAEs"
      ],
      "metadata": {
        "id": "0v7O6EQXOQOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ],
      "metadata": {
        "id": "tUtxpRz1P4ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8MJPsKJmGqF",
        "outputId": "080bc3a6-9608-4f46-96a6-ee1f733193e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install huggingface_hub datasets lightning einops bitsandbytes lpips"
      ],
      "metadata": {
        "id": "ff50ac_sQDu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b2b66d-9fc9-4d6e-f9a0-6cdb38cdecf5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/oelin/generative-models"
      ],
      "metadata": {
        "id": "evn5j0dl0Hd-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Implementation"
      ],
      "metadata": {
        "id": "jM2OuqRIOoXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Macros"
      ],
      "metadata": {
        "id": "gAzgkVIkIXi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.checkpoint import checkpoint_sequential\n",
        "\n",
        "\n",
        "def Convolution1x1(input_channels: int, output_channels: int) -> nn.Module:\n",
        "\n",
        "    return nn.Conv2d(\n",
        "        in_channels=input_channels,\n",
        "        out_channels=output_channels,\n",
        "        kernel_size=1,\n",
        "        stride=1,\n",
        "        padding=0,\n",
        "    )\n",
        "\n",
        "\n",
        "def Convolution3x3(input_channels: int, output_channels: int) -> nn.Module:\n",
        "\n",
        "    return nn.Conv2d(\n",
        "        in_channels=input_channels,\n",
        "        out_channels=output_channels,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        bias=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def Convolution4x4(input_channels: int, output_channels: int) -> nn.Module:\n",
        "\n",
        "    return nn.Conv2d(\n",
        "        in_channels=input_channels,\n",
        "        out_channels=output_channels,\n",
        "        kernel_size=4,\n",
        "        stride=2,\n",
        "        padding=1,\n",
        "        bias=False,\n",
        "    )\n",
        "\n",
        "\n",
        "def Normalization(channels: int) -> nn.Module:\n",
        "\n",
        "    return nn.GroupNorm(\n",
        "        num_groups=min(channels, 32),\n",
        "        num_channels=channels,\n",
        "    )\n",
        "\n",
        "\n",
        "def Repeat(module, channels_list: List[int]) -> nn.Module:\n",
        "\n",
        "    return nn.Sequential(*(\n",
        "        module(\n",
        "            input_channels=input_channels,\n",
        "            output_channels=output_channels,\n",
        "        ) for input_channels, output_channels in zip(\n",
        "            channels_list[: -1],\n",
        "            channels_list[1 :],\n",
        "        )\n",
        "    ))"
      ],
      "metadata": {
        "id": "v0UOjDsbcXIl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Modules"
      ],
      "metadata": {
        "id": "zAaxmRMfcVKr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EuH0adCXOE0o"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, channels: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.normalization = Normalization(channels=channels)\n",
        "\n",
        "        self.convolution = Convolution3x3(\n",
        "            input_channels=channels,\n",
        "            output_channels=channels,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        z = self.normalization(x)\n",
        "        z = F.leaky_relu(z)\n",
        "        z = self.convolution(z)\n",
        "\n",
        "        return x + z\n",
        "\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, channels: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.residual_block_1 = ResidualBlock(channels=channels)\n",
        "        self.residual_block_2 = ResidualBlock(channels=channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.residual_block_1(x)\n",
        "        x = self.residual_block_2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels: int, output_channels: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # self.upsample = nn.Upsample(scale_factor=2)\n",
        "        self.normalization = Normalization(channels=input_channels)\n",
        "\n",
        "        self.convolution = Convolution3x3(\n",
        "            input_channels=input_channels,\n",
        "            output_channels=output_channels,\n",
        "        )\n",
        "\n",
        "        # self.convolution = nn.ConvTranspose2d(\n",
        "        #     in_channels=input_channels,\n",
        "        #     out_channels=output_channels,\n",
        "        #     kernel_size=4,\n",
        "        #     stride=2,\n",
        "        #     padding=1,\n",
        "        # )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.normalization(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        # x = self.upsample(x)\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode='nearest')\n",
        "        x = self.convolution(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels: int, output_channels: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.normalization = Normalization(channels=input_channels)\n",
        "\n",
        "        self.convolution = Convolution4x4(\n",
        "            input_channels=input_channels,\n",
        "            output_channels=output_channels,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.normalization(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.convolution(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, channels: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.normalization = Normalization(channels=channels)\n",
        "\n",
        "        self.convolution_1 = Convolution1x1(\n",
        "            input_channels=channels,\n",
        "            output_channels=channels,\n",
        "        )\n",
        "\n",
        "        self.convolution_2 = Convolution1x1(\n",
        "            input_channels=channels,\n",
        "            output_channels=channels,\n",
        "        )\n",
        "\n",
        "        self.convolution_3 = Convolution1x1(\n",
        "            input_channels=channels,\n",
        "            output_channels=channels,\n",
        "        )\n",
        "\n",
        "        self.convolution_4 = Convolution1x1(\n",
        "            input_channels=channels,\n",
        "            output_channels=channels,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        z = self.normalization(x)\n",
        "        q = self.convolution_1(z)\n",
        "        k = self.convolution_2(z)\n",
        "        v = self.convolution_3(z)\n",
        "\n",
        "        q = rearrange(q, 'b c h w -> b (h w) c')\n",
        "        k = rearrange(k, 'b c h w -> b c (h w)')  # Transposed.\n",
        "        v = rearrange(v, 'b c h w -> b (h w) c')\n",
        "\n",
        "        z = F.softmax(q @ k, dim=-1) @ v\n",
        "        z = rearrange(z, 'b (h w) c -> b c h w', h=H, w=W)\n",
        "        z = self.convolution_4(z)\n",
        "\n",
        "        return x + z\n",
        "\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels: int, output_channels: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.resnet_block = ResNetBlock(channels=input_channels)\n",
        "\n",
        "        self.upsample_block = UpsampleBlock(\n",
        "            input_channels=input_channels,\n",
        "            output_channels=output_channels,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.resnet_block(x)\n",
        "        x = self.upsample_block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels: int, output_channels: int) ->  None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.resnet_block = ResNetBlock(channels=input_channels)\n",
        "\n",
        "        self.downsample_block = DownsampleBlock(\n",
        "            input_channels=input_channels,\n",
        "            output_channels=output_channels,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.resnet_block(x)\n",
        "        x = self.downsample_block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MiddleBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, channels: int) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.resnet_block_1 = ResNetBlock(channels=channels)\n",
        "        self.resnet_block_2 = ResNetBlock(channels=channels)\n",
        "        self.attention_block = AttentionBlock(channels=channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.resnet_block_1(x)\n",
        "        x = self.attention_block(x)\n",
        "        x = self.resnet_block_2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, channels_list: List[int]) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.down_blocks = Repeat(module=DownBlock, channels_list=channels_list)\n",
        "        self.middle_block = MiddleBlock(channels=channels_list[-1])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.down_blocks(x)\n",
        "        x = self.middle_block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, channels_list: List[int]) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.up_blocks = Repeat(module=UpBlock, channels_list=channels_list)\n",
        "        self.middle_block = MiddleBlock(channels=channels_list[0])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.middle_block(x)\n",
        "        x = self.up_blocks(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GaussianDistribution(nn.Module):\n",
        "\n",
        "    def __init__(self, parameters: torch.Tensor) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.mean, self.log_variance = parameters.chunk(chunks=2, dim=1)\n",
        "\n",
        "    def sample(self) -> torch.Tensor:\n",
        "\n",
        "        epsilon = torch.randn_like(self.mean, device=self.mean.device)\n",
        "        standard_deviation = torch.exp(0.5 * self.log_variance)\n",
        "        x = epsilon * standard_deviation + self.mean\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Models"
      ],
      "metadata": {
        "id": "3MdgfWwFX8ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class VAEOptions:\n",
        "\n",
        "    input_channels: int\n",
        "    output_channels: int\n",
        "    latent_channels: int\n",
        "    encoder_channels_list: List[int]\n",
        "    decoder_channels_list: List[int]\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "\n",
        "    def __init__(self, options: VAEOptions) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(channels_list=options.encoder_channels_list)\n",
        "        self.decoder = Decoder(channels_list=options.decoder_channels_list)\n",
        "\n",
        "        # Input to encoder.\n",
        "\n",
        "        self.convolution_1 = Convolution3x3(\n",
        "            input_channels=options.input_channels,\n",
        "            output_channels=options.encoder_channels_list[0],\n",
        "        )\n",
        "\n",
        "        # Encoder to latent.\n",
        "\n",
        "        self.convolution_2 = Convolution3x3(\n",
        "            input_channels=options.encoder_channels_list[-1],\n",
        "            output_channels=options.latent_channels * 2,\n",
        "        )\n",
        "\n",
        "        # Latent to decoder.\n",
        "\n",
        "        self.convolution_3 = Convolution3x3(\n",
        "            input_channels=options.latent_channels,\n",
        "            output_channels=options.decoder_channels_list[0],\n",
        "        )\n",
        "\n",
        "        # Decoder to output.\n",
        "\n",
        "        self.convolution_4 = Convolution3x3(\n",
        "            input_channels=options.decoder_channels_list[-1],\n",
        "            output_channels=options.output_channels,\n",
        "        )\n",
        "\n",
        "    def encode(self, x: torch.Tensor) -> GaussianDistribution:\n",
        "\n",
        "        x = self.convolution_1(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.convolution_2(x)\n",
        "\n",
        "        distribution = GaussianDistribution(x)\n",
        "\n",
        "        return distribution\n",
        "\n",
        "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.convolution_3(z)\n",
        "        x = self.decoder(x)\n",
        "        x = self.convolution_4(x)\n",
        "        # x = torch.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self,\n",
        "        x: torch.Tensor,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, GaussianDistribution]:\n",
        "\n",
        "        distribution = self.encode(x)\n",
        "        z = distribution.sample()\n",
        "        x = self.decode(z)\n",
        "\n",
        "        return x, z, distribution"
      ],
      "metadata": {
        "id": "b3BpZrP3X9Nm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class PatchDiscriminatorOptions:\n",
        "    input_channels: int\n",
        "    channels_list: int\n",
        "\n",
        "\n",
        "        # (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
        "        # (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        # (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "        # (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        # (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        # (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "        # (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        # (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        # (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        # (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        # (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        # (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, options: PatchDiscriminatorOptions) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.convolution_1 = Convolution3x3(\n",
        "            input_channels=options.input_channels,\n",
        "            output_channels=options.channels_list[0],\n",
        "        )\n",
        "\n",
        "        self.convolution_2 = Convolution3x3(\n",
        "            input_channels=options.channels_list[-1],\n",
        "            output_channels=1,\n",
        "        )\n",
        "\n",
        "        self.down_blocks = Repeat(\n",
        "            module=DownBlock,\n",
        "            channels_list=options.channels_list,\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        x = self.convolution_1(x)\n",
        "        x = self.down_blocks(x)\n",
        "        x = self.convolution_2(x)  # (b, 1, h/f, w/f).\n",
        "        x = torch.sigmoid(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "cNqfZ7lF2LN1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Losses"
      ],
      "metadata": {
        "id": "1gvL5wo3EYPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lpips\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class VAELossOptions:\n",
        "    kl_divergence_weight: float\n",
        "\n",
        "\n",
        "class VAELoss(nn.Module):\n",
        "\n",
        "    def __init__(self, options: VAELossOptions) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.kl_divergence_weight = options.kl_divergence_weight\n",
        "        self.lpips = lpips.LPIPS(net='vgg').eval()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        prediction: torch.Tensor,\n",
        "        distribution: GaussianDistribution,\n",
        "        target: torch.Tensor,\n",
        "    ) -> None:\n",
        "\n",
        "        # KL divergence loss.\n",
        "\n",
        "        loss = distribution.log_variance - distribution.log_variance.exp()\n",
        "        loss = loss - distribution.mean.pow(2) + 1\n",
        "        loss = loss.mean() * self.kl_divergence_weight * -0.5\n",
        "\n",
        "        # Perceptual loss.\n",
        "\n",
        "        loss = loss + self.lpips(prediction, target).mean() #F.binary_cross_entropy(prediction, target)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "I_LVDtAnEgL2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class VAEDiscriminatorLossOptions:\n",
        "\n",
        "    kl_divergence_weight: float\n",
        "    reconstruction_weight: float\n",
        "    perceptual_weight: float\n",
        "    generator_weight: float\n",
        "\n",
        "\n",
        "class VAEDiscriminatorLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, options: VAEDiscriminatorLossOptions) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.kl_divergence_weight = options.kl_divergence_weight\n",
        "        self.reconstruction_weight = options.reconstruction_weight\n",
        "        self.perceptual_weight = options.perceptual_weight\n",
        "        self.generator_weight = options.generator_weight\n",
        "\n",
        "        self.lpips = lpips.LPIPS(net='vgg').eval()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        reconstruction: torch.Tensor,\n",
        "        distribution: GaussianDistribution,\n",
        "        target: torch.Tensor,\n",
        "        last_layer: torch.Tensor,\n",
        "        discriminator: PatchDiscriminator,\n",
        "        use_discriminator: bool = False,\n",
        "        scale: float = 1.,\n",
        "    ) -> dict:\n",
        "\n",
        "        # KL divergence loss.\n",
        "\n",
        "        kl_divergence_loss = distribution.log_variance - distribution.log_variance.exp()\n",
        "        kl_divergence_loss = kl_divergence_loss - distribution.mean.pow(2) + 1\n",
        "        kl_divergence_loss = kl_divergence_loss.mean() * -0.5\n",
        "\n",
        "        # Reconstruction loss.\n",
        "\n",
        "        reconstruction_loss = torch.abs(reconstruction.contiguous() - target.contiguous()).mean()\n",
        "\n",
        "        # Perceptual loss.\n",
        "\n",
        "        perceptual_loss = self.lpips(reconstruction.contiguous(), target.contiguous()).mean()\n",
        "\n",
        "        # Generator loss.\n",
        "\n",
        "        if use_discriminator:#\n",
        "            generator_loss = -torch.log(discriminator(reconstruction).mean()) # Maximize p(real) on fake inputs.\n",
        "\n",
        "            # Compute adaptive weight: the ratio between the reconstruction and generator gradients.\n",
        "\n",
        "            reconstruction_gradient = torch.autograd.grad(reconstruction_loss, last_layer, retain_graph=True)[0]\n",
        "            generator_gradient = torch.autograd.grad(generator_loss, last_layer, retain_graph=True)[0]\n",
        "\n",
        "            ratio = torch.norm(reconstruction_gradient) / (torch.norm(generator_gradient) + 1e-4)\n",
        "            ratio = torch.clamp(ratio, 0., 1e4).detach()\n",
        "\n",
        "            generator_loss = generator_loss #* ratio  # Scale generator loss accordingly.\n",
        "\n",
        "        else:\n",
        "            generator_loss = 0.\n",
        "\n",
        "        # Overall loss.\n",
        "\n",
        "        loss = self.kl_divergence_weight * kl_divergence_loss \\\n",
        "             + self.reconstruction_weight * reconstruction_loss \\\n",
        "             + self.perceptual_weight * perceptual_loss \\\n",
        "             + self.generator_weight * generator_loss\n",
        "\n",
        "        return {\n",
        "            'loss': loss * scale,\n",
        "            'kl_divergence_loss': kl_divergence_loss * self.kl_divergence_weight,\n",
        "            'reconstruction_loss': reconstruction_loss * self.reconstruction_weight,\n",
        "            'perceptual_loss': perceptual_loss * self.perceptual_weight,\n",
        "            'generator_loss': generator_loss * self.generator_weight,\n",
        "        }"
      ],
      "metadata": {
        "id": "oeaHvKAN-kaX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training"
      ],
      "metadata": {
        "id": "IVNWmYQV-vGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Training Script"
      ],
      "metadata": {
        "id": "2Y1SpqxbSz3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "from typing import Callable, Optional\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TrainOptions:\n",
        "    device: str\n",
        "\n",
        "    epochs: int\n",
        "    batch_size: int\n",
        "    generator_learning_rate: float\n",
        "    discriminator_learning_rate: float\n",
        "    discriminator_warmup_steps: int\n",
        "\n",
        "    batches_before_step: int\n",
        "    batches_before_log: int\n",
        "    batches_before_sample: int\n",
        "    batches_before_checkpoint: int\n",
        "\n",
        "    checkpoint_path: str\n",
        "    sample_path: str\n",
        "\n",
        "    generator_options: VAEOptions\n",
        "    discriminator_options: PatchDiscriminatorOptions\n",
        "    loss_options: VAEDiscriminatorLossOptions\n",
        "\n",
        "    generator: Optional[VAE] = None\n",
        "    discriminator: Optional[PatchDiscriminator] = None\n",
        "    loss: Optional[VAEDiscriminatorLoss] = None\n",
        "\n",
        "\n",
        "def train_summary(\n",
        "    options: TrainOptions,\n",
        "    generator: VAE,\n",
        "    discriminator: PatchDiscriminator,\n",
        "    log: Callable=print,\n",
        ") -> None:\n",
        "\n",
        "    log(f'==========')\n",
        "\n",
        "    log(f'Training for {options.epochs} epochs on {options.device}...')\n",
        "\n",
        "    generator_size = sum(p.numel() for p in generator.parameters())\n",
        "    generator_f_value = 2 ** (len(options.generator_options.encoder_channels_list) - 1)\n",
        "    generator_c_value = options.generator_options.latent_channels\n",
        "\n",
        "    log(f'Generator:')\n",
        "    log(f'- Parameters: {generator_size/1e6:0.2f}M')\n",
        "    log(f'- Downsampling factor: {generator_f_value}')\n",
        "    log(f'- Bottleneck channels: {generator_c_value}')\n",
        "    log(f'- Optimizer: Adam (learning_rate={options.generator_learning_rate})')\n",
        "\n",
        "    # discriminator_size = sum(p.numel() for p in discriminator.parameters())\n",
        "    # discriminator_f_value = 2 ** (len(options.discriminator_options.channels_list) - 1)\n",
        "\n",
        "    # log(f'Discriminator:')\n",
        "    # log(f'- Parameters: {discriminator_size/1e6:0.2f}M')\n",
        "    # log(f'- Downsampling factor: {generator_f_value}')\n",
        "    # log(f'- Optimizer: Adam (learning_rate={options.discriminator_learning_rate})')\n",
        "\n",
        "    log(f'Loss:')\n",
        "    log(f'- KL divergence weight: {options.loss_options.kl_divergence_weight}')\n",
        "    log(f'- Reconstruction weight: {options.loss_options.reconstruction_weight}')\n",
        "    log(f'- Perceptual weight: {options.loss_options.perceptual_weight}')\n",
        "    log(f'- Generator weight: {options.loss_options.generator_weight}')\n",
        "\n",
        "    log(f'Training:')\n",
        "    log(f'- Epochs: {options.epochs} (batch_size={options.batch_size})')\n",
        "    log(f'- Batches per step: {options.batches_before_step}')\n",
        "    log(f'- Batches per checkpoint: {options.batches_before_checkpoint}')\n",
        "    log(f'- Discriminator warmup steps: {options.discriminator_warmup_steps}')\n",
        "\n",
        "    log(f'==========')\n",
        "\n",
        "def train(\n",
        "    options: TrainOptions,\n",
        "    dataset: Dataset,\n",
        "    log: Callable=print,\n",
        ") -> TrainOptions:\n",
        "\n",
        "    # Initialize models.\n",
        "\n",
        "    device = options.device\n",
        "\n",
        "    generator = options.generator or VAE(options=options.generator_options)\n",
        "\n",
        "    # discriminator = options.discriminator \\\n",
        "    #     or PatchDiscriminator(options=options.discriminator_options)\n",
        "\n",
        "    loss = options.loss or VAEDiscriminatorLoss(options=options.loss_options)\n",
        "\n",
        "    generator = generator.to(device)\n",
        "    #discriminator = discriminator.to(device)\n",
        "    loss = loss.to(device)\n",
        "\n",
        "    # Initialize dataloaders.\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        shuffle=True,\n",
        "        batch_size=options.batch_size,\n",
        "        num_workers=2,\n",
        "    )\n",
        "\n",
        "    # Initialize optimizers.\n",
        "\n",
        "    generator_optimizer = bnb.optim.Adam8bit(generator.parameters(), lr=options.generator_learning_rate)#Adam(generator.parameters(), lr=options.generator_learning_rate)\n",
        "    #discriminator_optimizer = Adam(discriminator.parameters(), lr=options.discriminator_learning_rate)\n",
        "\n",
        "    # Display summary before training.\n",
        "\n",
        "    train_summary(options, generator, None, log)#discriminator, log)\n",
        "\n",
        "    # Start training loop.\n",
        "\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(options.epochs):\n",
        "        for batch, examples in enumerate(dataloader):\n",
        "\n",
        "            use_discriminator = step >= options.discriminator_warmup_steps\n",
        "\n",
        "            target = examples['image'].to(device)\n",
        "\n",
        "            with torch.autocast(device_type='cuda'): #device):\n",
        "                reconstruction, _, distribution = generator(target)\n",
        "\n",
        "                loss_on_batch = loss(\n",
        "                    reconstruction=reconstruction,\n",
        "                    distribution=distribution,\n",
        "                    target=target,\n",
        "                    last_layer=generator.convolution_4.weight,  # Last layer of decoder.\n",
        "                    discriminator=None,\n",
        "                    use_discriminator=use_discriminator,\n",
        "                    scale=1/options.batches_before_step,\n",
        "                )\n",
        "\n",
        "            loss_on_batch['loss'].backward()\n",
        "            discriminator_loss = None\n",
        "\n",
        "            if (batch + 1) % options.batches_before_step == 0:\n",
        "                generator_optimizer.step()\n",
        "                generator_optimizer.zero_grad()\n",
        "\n",
        "                if use_discriminator:\n",
        "\n",
        "                    discriminator_optimizer.zero_grad()\n",
        "\n",
        "                    p_real_real = discriminator(target.detach()).mean()\n",
        "                    p_real_fake = discriminator(reconstruction.detach()).mean()\n",
        "\n",
        "                    loss_real = -torch.log(p_real_real)\n",
        "                    loss_fake = -torch.log(1 - p_real_fake)\n",
        "\n",
        "                    discriminator_loss = loss_real + loss_fake\n",
        "                    discriminator_loss.backward()\n",
        "\n",
        "                    discriminator_optimizer.step()\n",
        "\n",
        "                step += 1\n",
        "\n",
        "            if (batch + 1) % options.batches_before_log == 0:\n",
        "\n",
        "                loss1 = loss_on_batch['loss'].detach().item()\n",
        "                loss2 = loss_on_batch['kl_divergence_loss'].detach().item()\n",
        "                loss3 = loss_on_batch['reconstruction_loss'].detach().item()\n",
        "                loss4 = loss_on_batch['perceptual_loss'].detach().item()\n",
        "                loss5 = f'{loss_on_batch[\"generator_loss\"].detach().item():0.3f}' if use_discriminator else 'n/a'\n",
        "                loss6 = f'{discriminator_loss.detach().item():0.3f}' if use_discriminator else 'n/a'\n",
        "                loss7 = f'{p_real_real.detach().item():0.5f}' if use_discriminator else 'n/a'\n",
        "                loss8 = f'{p_real_fake.detach().item():0.5f}' if use_discriminator else 'n/a'\n",
        "\n",
        "                log(f'{time.ctime()} | epoch: {epoch:6d}, batch: {batch:6d}, step: {step:6d} - loss: {loss1:0.3f} (kld={loss2:0.3f}, rec={loss3:0.3f}, per={loss4:0.3f}, gen={loss5}, dis={loss6}), p(R|R)={loss7}, p(R|F)={loss8}')\n",
        "\n",
        "            if (batch + 1) % options.batches_before_sample == 0:\n",
        "                save_image(rescale(reconstruction)[: 64], f'{options.sample_path}/sample.png')#/sample-{step}.png')\n",
        "\n",
        "            if (batch + 1) % options.batches_before_checkpoint == 0:\n",
        "                log('Creating checkpoint...')\n",
        "\n",
        "                torch.save(generator, f'{options.checkpoint_path}/generator.ckpt')#generator-{step}.ckpt')"
      ],
      "metadata": {
        "id": "JrRPFMDPJm6u"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. CelebA-HQ-256x256"
      ],
      "metadata": {
        "id": "aA5HueTvJBBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "resolution = 256 #512\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: 2 * (x - 0.5)),\n",
        "    #transforms.Resize((resolution, resolution))#, 0)\n",
        "])\n",
        "\n",
        "\n",
        "def preprocess(examples):\n",
        "\n",
        "  return {\n",
        "      'image': [transform(image) for image in examples['image']]\n",
        "  }\n",
        "\n",
        "\n",
        "def rescale(x):\n",
        "    return (x - x.min()) / (x.max() - x.min())\n",
        "\n",
        "\n",
        "dataset = load_dataset('korexyz/CelebA-HQ-256x256', split='train')\n",
        "#dataset = load_dataset('mattymchen/celeba-hq', split='train')\n",
        "dataset.set_transform(transform=preprocess)"
      ],
      "metadata": {
        "id": "h0nTA3_OpO91"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"backend:native,roundup_power2_divisions:1024\"\n",
        "\n",
        "\n",
        "generator_options = VAEOptions(\n",
        "    input_channels=3,\n",
        "    output_channels=3,\n",
        "    latent_channels=4,\n",
        "    encoder_channels_list=[128, 256, 512],\n",
        "    decoder_channels_list=[512, 256, 128],\n",
        ")\n",
        "\n",
        "discriminator_options = PatchDiscriminatorOptions(\n",
        "    input_channels=generator_options.output_channels,\n",
        "    channels_list=[32, 32, 32],\n",
        ")\n",
        "\n",
        "loss_options = VAEDiscriminatorLossOptions(\n",
        "    kl_divergence_weight=1e-4,#1e-6,\n",
        "    reconstruction_weight=1.,#1.,#1.,\n",
        "    perceptual_weight=0.25,#0.,#1.,#1.,\n",
        "    generator_weight=0.5,\n",
        ")\n",
        "\n",
        "loss = VAEDiscriminatorLoss(options=loss_options)\n",
        "\n",
        "#generator = torch.load('/content/drive/MyDrive/generator-day2-320.ckpt')\n",
        "\n",
        "#generator = VAE(options=generator_options)\n",
        "#discriminator = PatchDiscriminator(options=discriminator_options)\n",
        "\n",
        "\n",
        "train_options = TrainOptions(\n",
        "\n",
        "    device='cuda',\n",
        "    epochs=2,\n",
        "    batch_size=1,#4,\n",
        "    generator_learning_rate=1e-5,\n",
        "    discriminator_learning_rate=1e-5,\n",
        "    discriminator_warmup_steps=1000,\n",
        "\n",
        "    batches_before_step=8,#4,\n",
        "    batches_before_log=16,#8,\n",
        "    batches_before_sample=64,#32,\n",
        "    batches_before_checkpoint=128,#128,\n",
        "\n",
        "    checkpoint_path='./checkpoints',\n",
        "    sample_path='./samples',\n",
        "\n",
        "    generator_options=generator_options,\n",
        "    discriminator_options=discriminator_options,\n",
        "    loss_options=loss_options,\n",
        "\n",
        "    generator=generator,\n",
        "    discriminator=None,\n",
        "    loss=loss,\n",
        ")\n",
        "\n",
        "\n",
        "!rm -rf checkpoints samples\n",
        "!mkdir checkpoints samples\n",
        "\n",
        "train(options=train_options, dataset=dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9E55f7BJFSh",
        "outputId": "0f164b71-8a26-4d3a-cbbe-99f612a39d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/vgg.pth\n",
            "==========\n",
            "Training for 2 epochs on cuda...\n",
            "Generator:\n",
            "- Parameters: 32.52M\n",
            "- Downsampling factor: 4\n",
            "- Bottleneck channels: 4\n",
            "- Optimizer: Adam (learning_rate=1e-05)\n",
            "Loss:\n",
            "- KL divergence weight: 0.0001\n",
            "- Reconstruction weight: 1.0\n",
            "- Perceptual weight: 0.25\n",
            "- Generator weight: 0.5\n",
            "Training:\n",
            "- Epochs: 2 (batch_size=1)\n",
            "- Batches per step: 8\n",
            "- Batches per checkpoint: 128\n",
            "- Discriminator warmup steps: 1000\n",
            "==========\n",
            "Wed Dec 20 13:25:55 2023 | epoch:      0, batch:     15, step:      2 - loss: 0.047 (kld=0.000, rec=0.230, per=0.144, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:25:57 2023 | epoch:      0, batch:     31, step:      4 - loss: 0.034 (kld=0.000, rec=0.136, per=0.135, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:25:59 2023 | epoch:      0, batch:     47, step:      6 - loss: 0.028 (kld=0.000, rec=0.121, per=0.102, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:02 2023 | epoch:      0, batch:     63, step:      8 - loss: 0.028 (kld=0.000, rec=0.115, per=0.108, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:04 2023 | epoch:      0, batch:     79, step:     10 - loss: 0.025 (kld=0.000, rec=0.084, per=0.111, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:06 2023 | epoch:      0, batch:     95, step:     12 - loss: 0.027 (kld=0.000, rec=0.111, per=0.102, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:08 2023 | epoch:      0, batch:    111, step:     14 - loss: 0.023 (kld=0.000, rec=0.088, per=0.094, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:11 2023 | epoch:      0, batch:    127, step:     16 - loss: 0.024 (kld=0.001, rec=0.084, per=0.108, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:26:13 2023 | epoch:      0, batch:    143, step:     18 - loss: 0.025 (kld=0.000, rec=0.092, per=0.110, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:16 2023 | epoch:      0, batch:    159, step:     20 - loss: 0.028 (kld=0.000, rec=0.116, per=0.109, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:18 2023 | epoch:      0, batch:    175, step:     22 - loss: 0.021 (kld=0.000, rec=0.062, per=0.102, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:20 2023 | epoch:      0, batch:    191, step:     24 - loss: 0.027 (kld=0.001, rec=0.114, per=0.099, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:23 2023 | epoch:      0, batch:    207, step:     26 - loss: 0.022 (kld=0.000, rec=0.079, per=0.095, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:25 2023 | epoch:      0, batch:    223, step:     28 - loss: 0.021 (kld=0.001, rec=0.071, per=0.099, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:27 2023 | epoch:      0, batch:    239, step:     30 - loss: 0.024 (kld=0.001, rec=0.100, per=0.095, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:30 2023 | epoch:      0, batch:    255, step:     32 - loss: 0.018 (kld=0.001, rec=0.058, per=0.087, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:26:33 2023 | epoch:      0, batch:    271, step:     34 - loss: 0.018 (kld=0.000, rec=0.057, per=0.088, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:35 2023 | epoch:      0, batch:    287, step:     36 - loss: 0.030 (kld=0.000, rec=0.130, per=0.112, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:37 2023 | epoch:      0, batch:    303, step:     38 - loss: 0.024 (kld=0.001, rec=0.086, per=0.104, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:39 2023 | epoch:      0, batch:    319, step:     40 - loss: 0.021 (kld=0.001, rec=0.065, per=0.099, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:42 2023 | epoch:      0, batch:    335, step:     42 - loss: 0.022 (kld=0.001, rec=0.072, per=0.100, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:44 2023 | epoch:      0, batch:    351, step:     44 - loss: 0.021 (kld=0.001, rec=0.083, per=0.087, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:46 2023 | epoch:      0, batch:    367, step:     46 - loss: 0.023 (kld=0.001, rec=0.071, per=0.109, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:48 2023 | epoch:      0, batch:    383, step:     48 - loss: 0.019 (kld=0.001, rec=0.060, per=0.092, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:26:51 2023 | epoch:      0, batch:    399, step:     50 - loss: 0.025 (kld=0.000, rec=0.097, per=0.101, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:53 2023 | epoch:      0, batch:    415, step:     52 - loss: 0.027 (kld=0.001, rec=0.093, per=0.119, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:55 2023 | epoch:      0, batch:    431, step:     54 - loss: 0.020 (kld=0.001, rec=0.073, per=0.089, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:26:57 2023 | epoch:      0, batch:    447, step:     56 - loss: 0.021 (kld=0.001, rec=0.077, per=0.089, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:00 2023 | epoch:      0, batch:    463, step:     58 - loss: 0.044 (kld=0.000, rec=0.245, per=0.110, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:02 2023 | epoch:      0, batch:    479, step:     60 - loss: 0.024 (kld=0.001, rec=0.082, per=0.106, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:04 2023 | epoch:      0, batch:    495, step:     62 - loss: 0.020 (kld=0.001, rec=0.070, per=0.086, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:06 2023 | epoch:      0, batch:    511, step:     64 - loss: 0.026 (kld=0.001, rec=0.104, per=0.102, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:27:09 2023 | epoch:      0, batch:    527, step:     66 - loss: 0.020 (kld=0.001, rec=0.072, per=0.089, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:11 2023 | epoch:      0, batch:    543, step:     68 - loss: 0.020 (kld=0.001, rec=0.071, per=0.089, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:13 2023 | epoch:      0, batch:    559, step:     70 - loss: 0.020 (kld=0.001, rec=0.071, per=0.085, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:16 2023 | epoch:      0, batch:    575, step:     72 - loss: 0.017 (kld=0.001, rec=0.057, per=0.080, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:18 2023 | epoch:      0, batch:    591, step:     74 - loss: 0.022 (kld=0.001, rec=0.076, per=0.102, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:20 2023 | epoch:      0, batch:    607, step:     76 - loss: 0.018 (kld=0.001, rec=0.055, per=0.088, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:22 2023 | epoch:      0, batch:    623, step:     78 - loss: 0.020 (kld=0.001, rec=0.075, per=0.087, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:25 2023 | epoch:      0, batch:    639, step:     80 - loss: 0.019 (kld=0.001, rec=0.072, per=0.083, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:27:27 2023 | epoch:      0, batch:    655, step:     82 - loss: 0.019 (kld=0.001, rec=0.064, per=0.089, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:29 2023 | epoch:      0, batch:    671, step:     84 - loss: 0.024 (kld=0.001, rec=0.087, per=0.106, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:32 2023 | epoch:      0, batch:    687, step:     86 - loss: 0.021 (kld=0.001, rec=0.070, per=0.099, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:34 2023 | epoch:      0, batch:    703, step:     88 - loss: 0.021 (kld=0.001, rec=0.063, per=0.106, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:36 2023 | epoch:      0, batch:    719, step:     90 - loss: 0.019 (kld=0.001, rec=0.073, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:38 2023 | epoch:      0, batch:    735, step:     92 - loss: 0.017 (kld=0.001, rec=0.059, per=0.080, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:41 2023 | epoch:      0, batch:    751, step:     94 - loss: 0.021 (kld=0.001, rec=0.080, per=0.087, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:43 2023 | epoch:      0, batch:    767, step:     96 - loss: 0.015 (kld=0.001, rec=0.043, per=0.072, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:27:46 2023 | epoch:      0, batch:    783, step:     98 - loss: 0.022 (kld=0.001, rec=0.082, per=0.090, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:48 2023 | epoch:      0, batch:    799, step:    100 - loss: 0.017 (kld=0.001, rec=0.056, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:50 2023 | epoch:      0, batch:    815, step:    102 - loss: 0.018 (kld=0.001, rec=0.063, per=0.079, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:52 2023 | epoch:      0, batch:    831, step:    104 - loss: 0.018 (kld=0.001, rec=0.059, per=0.084, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:55 2023 | epoch:      0, batch:    847, step:    106 - loss: 0.016 (kld=0.001, rec=0.044, per=0.085, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:57 2023 | epoch:      0, batch:    863, step:    108 - loss: 0.017 (kld=0.001, rec=0.062, per=0.073, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:27:59 2023 | epoch:      0, batch:    879, step:    110 - loss: 0.019 (kld=0.001, rec=0.068, per=0.082, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:01 2023 | epoch:      0, batch:    895, step:    112 - loss: 0.020 (kld=0.001, rec=0.073, per=0.089, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:28:04 2023 | epoch:      0, batch:    911, step:    114 - loss: 0.021 (kld=0.001, rec=0.084, per=0.087, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:06 2023 | epoch:      0, batch:    927, step:    116 - loss: 0.017 (kld=0.001, rec=0.057, per=0.082, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:09 2023 | epoch:      0, batch:    943, step:    118 - loss: 0.020 (kld=0.001, rec=0.067, per=0.090, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:11 2023 | epoch:      0, batch:    959, step:    120 - loss: 0.019 (kld=0.001, rec=0.064, per=0.088, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:13 2023 | epoch:      0, batch:    975, step:    122 - loss: 0.018 (kld=0.001, rec=0.063, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:15 2023 | epoch:      0, batch:    991, step:    124 - loss: 0.022 (kld=0.001, rec=0.091, per=0.086, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:18 2023 | epoch:      0, batch:   1007, step:    126 - loss: 0.024 (kld=0.001, rec=0.095, per=0.094, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:20 2023 | epoch:      0, batch:   1023, step:    128 - loss: 0.021 (kld=0.001, rec=0.080, per=0.084, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:28:22 2023 | epoch:      0, batch:   1039, step:    130 - loss: 0.024 (kld=0.001, rec=0.101, per=0.087, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:25 2023 | epoch:      0, batch:   1055, step:    132 - loss: 0.028 (kld=0.001, rec=0.132, per=0.088, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:27 2023 | epoch:      0, batch:   1071, step:    134 - loss: 0.021 (kld=0.001, rec=0.058, per=0.105, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:29 2023 | epoch:      0, batch:   1087, step:    136 - loss: 0.023 (kld=0.001, rec=0.091, per=0.091, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:31 2023 | epoch:      0, batch:   1103, step:    138 - loss: 0.020 (kld=0.001, rec=0.077, per=0.082, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:34 2023 | epoch:      0, batch:   1119, step:    140 - loss: 0.016 (kld=0.001, rec=0.058, per=0.067, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:36 2023 | epoch:      0, batch:   1135, step:    142 - loss: 0.019 (kld=0.001, rec=0.067, per=0.081, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:38 2023 | epoch:      0, batch:   1151, step:    144 - loss: 0.023 (kld=0.001, rec=0.084, per=0.100, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:28:41 2023 | epoch:      0, batch:   1167, step:    146 - loss: 0.021 (kld=0.001, rec=0.082, per=0.082, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:43 2023 | epoch:      0, batch:   1183, step:    148 - loss: 0.017 (kld=0.001, rec=0.050, per=0.085, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:45 2023 | epoch:      0, batch:   1199, step:    150 - loss: 0.016 (kld=0.001, rec=0.053, per=0.076, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:47 2023 | epoch:      0, batch:   1215, step:    152 - loss: 0.023 (kld=0.001, rec=0.087, per=0.098, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:50 2023 | epoch:      0, batch:   1231, step:    154 - loss: 0.016 (kld=0.001, rec=0.060, per=0.069, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:52 2023 | epoch:      0, batch:   1247, step:    156 - loss: 0.017 (kld=0.001, rec=0.056, per=0.077, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:54 2023 | epoch:      0, batch:   1263, step:    158 - loss: 0.022 (kld=0.001, rec=0.096, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:28:56 2023 | epoch:      0, batch:   1279, step:    160 - loss: 0.021 (kld=0.001, rec=0.077, per=0.093, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:28:59 2023 | epoch:      0, batch:   1295, step:    162 - loss: 0.018 (kld=0.001, rec=0.063, per=0.083, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:01 2023 | epoch:      0, batch:   1311, step:    164 - loss: 0.015 (kld=0.001, rec=0.052, per=0.069, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:04 2023 | epoch:      0, batch:   1327, step:    166 - loss: 0.023 (kld=0.001, rec=0.092, per=0.090, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:06 2023 | epoch:      0, batch:   1343, step:    168 - loss: 0.020 (kld=0.001, rec=0.083, per=0.079, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:08 2023 | epoch:      0, batch:   1359, step:    170 - loss: 0.021 (kld=0.001, rec=0.080, per=0.090, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:10 2023 | epoch:      0, batch:   1375, step:    172 - loss: 0.015 (kld=0.001, rec=0.044, per=0.075, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:13 2023 | epoch:      0, batch:   1391, step:    174 - loss: 0.021 (kld=0.001, rec=0.083, per=0.088, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:15 2023 | epoch:      0, batch:   1407, step:    176 - loss: 0.020 (kld=0.001, rec=0.074, per=0.089, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:29:17 2023 | epoch:      0, batch:   1423, step:    178 - loss: 0.017 (kld=0.001, rec=0.064, per=0.071, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:20 2023 | epoch:      0, batch:   1439, step:    180 - loss: 0.021 (kld=0.001, rec=0.075, per=0.090, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:22 2023 | epoch:      0, batch:   1455, step:    182 - loss: 0.015 (kld=0.001, rec=0.053, per=0.066, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:24 2023 | epoch:      0, batch:   1471, step:    184 - loss: 0.015 (kld=0.001, rec=0.052, per=0.064, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:26 2023 | epoch:      0, batch:   1487, step:    186 - loss: 0.020 (kld=0.001, rec=0.067, per=0.091, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:29 2023 | epoch:      0, batch:   1503, step:    188 - loss: 0.021 (kld=0.001, rec=0.081, per=0.084, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:31 2023 | epoch:      0, batch:   1519, step:    190 - loss: 0.014 (kld=0.001, rec=0.050, per=0.064, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:33 2023 | epoch:      0, batch:   1535, step:    192 - loss: 0.019 (kld=0.001, rec=0.067, per=0.081, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:29:36 2023 | epoch:      0, batch:   1551, step:    194 - loss: 0.015 (kld=0.001, rec=0.052, per=0.069, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:38 2023 | epoch:      0, batch:   1567, step:    196 - loss: 0.016 (kld=0.001, rec=0.060, per=0.070, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:40 2023 | epoch:      0, batch:   1583, step:    198 - loss: 0.016 (kld=0.001, rec=0.049, per=0.079, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:42 2023 | epoch:      0, batch:   1599, step:    200 - loss: 0.017 (kld=0.001, rec=0.066, per=0.073, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:45 2023 | epoch:      0, batch:   1615, step:    202 - loss: 0.022 (kld=0.001, rec=0.078, per=0.094, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:47 2023 | epoch:      0, batch:   1631, step:    204 - loss: 0.018 (kld=0.001, rec=0.066, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:49 2023 | epoch:      0, batch:   1647, step:    206 - loss: 0.015 (kld=0.001, rec=0.050, per=0.072, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:51 2023 | epoch:      0, batch:   1663, step:    208 - loss: 0.013 (kld=0.001, rec=0.041, per=0.063, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:29:54 2023 | epoch:      0, batch:   1679, step:    210 - loss: 0.017 (kld=0.001, rec=0.064, per=0.070, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:56 2023 | epoch:      0, batch:   1695, step:    212 - loss: 0.022 (kld=0.001, rec=0.085, per=0.089, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:29:58 2023 | epoch:      0, batch:   1711, step:    214 - loss: 0.018 (kld=0.001, rec=0.072, per=0.070, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:01 2023 | epoch:      0, batch:   1727, step:    216 - loss: 0.016 (kld=0.001, rec=0.055, per=0.069, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:03 2023 | epoch:      0, batch:   1743, step:    218 - loss: 0.018 (kld=0.001, rec=0.068, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:05 2023 | epoch:      0, batch:   1759, step:    220 - loss: 0.019 (kld=0.001, rec=0.067, per=0.084, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:07 2023 | epoch:      0, batch:   1775, step:    222 - loss: 0.017 (kld=0.001, rec=0.065, per=0.070, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:10 2023 | epoch:      0, batch:   1791, step:    224 - loss: 0.018 (kld=0.001, rec=0.066, per=0.081, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:30:12 2023 | epoch:      0, batch:   1807, step:    226 - loss: 0.018 (kld=0.001, rec=0.067, per=0.075, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:15 2023 | epoch:      0, batch:   1823, step:    228 - loss: 0.017 (kld=0.001, rec=0.051, per=0.083, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:17 2023 | epoch:      0, batch:   1839, step:    230 - loss: 0.016 (kld=0.001, rec=0.048, per=0.076, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:19 2023 | epoch:      0, batch:   1855, step:    232 - loss: 0.019 (kld=0.001, rec=0.071, per=0.081, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:21 2023 | epoch:      0, batch:   1871, step:    234 - loss: 0.015 (kld=0.001, rec=0.059, per=0.061, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:24 2023 | epoch:      0, batch:   1887, step:    236 - loss: 0.022 (kld=0.001, rec=0.079, per=0.094, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:26 2023 | epoch:      0, batch:   1903, step:    238 - loss: 0.016 (kld=0.001, rec=0.058, per=0.069, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:28 2023 | epoch:      0, batch:   1919, step:    240 - loss: 0.023 (kld=0.001, rec=0.089, per=0.091, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:30:31 2023 | epoch:      0, batch:   1935, step:    242 - loss: 0.014 (kld=0.001, rec=0.046, per=0.067, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:33 2023 | epoch:      0, batch:   1951, step:    244 - loss: 0.018 (kld=0.001, rec=0.067, per=0.074, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:35 2023 | epoch:      0, batch:   1967, step:    246 - loss: 0.018 (kld=0.001, rec=0.068, per=0.077, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:37 2023 | epoch:      0, batch:   1983, step:    248 - loss: 0.020 (kld=0.001, rec=0.086, per=0.076, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:40 2023 | epoch:      0, batch:   1999, step:    250 - loss: 0.020 (kld=0.001, rec=0.063, per=0.093, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:42 2023 | epoch:      0, batch:   2015, step:    252 - loss: 0.016 (kld=0.001, rec=0.053, per=0.074, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:44 2023 | epoch:      0, batch:   2031, step:    254 - loss: 0.013 (kld=0.001, rec=0.042, per=0.062, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:46 2023 | epoch:      0, batch:   2047, step:    256 - loss: 0.017 (kld=0.001, rec=0.064, per=0.073, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:30:49 2023 | epoch:      0, batch:   2063, step:    258 - loss: 0.017 (kld=0.001, rec=0.063, per=0.074, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:51 2023 | epoch:      0, batch:   2079, step:    260 - loss: 0.016 (kld=0.001, rec=0.065, per=0.064, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:53 2023 | epoch:      0, batch:   2095, step:    262 - loss: 0.012 (kld=0.001, rec=0.042, per=0.054, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:56 2023 | epoch:      0, batch:   2111, step:    264 - loss: 0.022 (kld=0.001, rec=0.079, per=0.095, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:30:58 2023 | epoch:      0, batch:   2127, step:    266 - loss: 0.018 (kld=0.001, rec=0.068, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:00 2023 | epoch:      0, batch:   2143, step:    268 - loss: 0.017 (kld=0.001, rec=0.053, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:02 2023 | epoch:      0, batch:   2159, step:    270 - loss: 0.017 (kld=0.001, rec=0.053, per=0.080, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:05 2023 | epoch:      0, batch:   2175, step:    272 - loss: 0.016 (kld=0.001, rec=0.057, per=0.068, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:31:07 2023 | epoch:      0, batch:   2191, step:    274 - loss: 0.020 (kld=0.001, rec=0.079, per=0.081, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:10 2023 | epoch:      0, batch:   2207, step:    276 - loss: 0.019 (kld=0.001, rec=0.064, per=0.083, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:12 2023 | epoch:      0, batch:   2223, step:    278 - loss: 0.014 (kld=0.001, rec=0.048, per=0.060, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:14 2023 | epoch:      0, batch:   2239, step:    280 - loss: 0.020 (kld=0.001, rec=0.073, per=0.086, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:16 2023 | epoch:      0, batch:   2255, step:    282 - loss: 0.014 (kld=0.001, rec=0.046, per=0.068, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:19 2023 | epoch:      0, batch:   2271, step:    284 - loss: 0.014 (kld=0.001, rec=0.045, per=0.065, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:21 2023 | epoch:      0, batch:   2287, step:    286 - loss: 0.015 (kld=0.001, rec=0.053, per=0.067, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:23 2023 | epoch:      0, batch:   2303, step:    288 - loss: 0.024 (kld=0.001, rec=0.099, per=0.095, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:31:26 2023 | epoch:      0, batch:   2319, step:    290 - loss: 0.025 (kld=0.001, rec=0.105, per=0.091, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:28 2023 | epoch:      0, batch:   2335, step:    292 - loss: 0.019 (kld=0.001, rec=0.059, per=0.093, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:30 2023 | epoch:      0, batch:   2351, step:    294 - loss: 0.023 (kld=0.001, rec=0.093, per=0.088, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:32 2023 | epoch:      0, batch:   2367, step:    296 - loss: 0.018 (kld=0.001, rec=0.057, per=0.084, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:35 2023 | epoch:      0, batch:   2383, step:    298 - loss: 0.017 (kld=0.001, rec=0.058, per=0.074, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:37 2023 | epoch:      0, batch:   2399, step:    300 - loss: 0.017 (kld=0.001, rec=0.054, per=0.079, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:39 2023 | epoch:      0, batch:   2415, step:    302 - loss: 0.016 (kld=0.001, rec=0.053, per=0.073, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:41 2023 | epoch:      0, batch:   2431, step:    304 - loss: 0.020 (kld=0.001, rec=0.067, per=0.090, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:31:44 2023 | epoch:      0, batch:   2447, step:    306 - loss: 0.013 (kld=0.001, rec=0.034, per=0.067, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:46 2023 | epoch:      0, batch:   2463, step:    308 - loss: 0.016 (kld=0.001, rec=0.056, per=0.067, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:49 2023 | epoch:      0, batch:   2479, step:    310 - loss: 0.016 (kld=0.001, rec=0.057, per=0.071, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:51 2023 | epoch:      0, batch:   2495, step:    312 - loss: 0.016 (kld=0.001, rec=0.064, per=0.066, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:53 2023 | epoch:      0, batch:   2511, step:    314 - loss: 0.017 (kld=0.001, rec=0.053, per=0.082, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:55 2023 | epoch:      0, batch:   2527, step:    316 - loss: 0.020 (kld=0.001, rec=0.081, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:31:58 2023 | epoch:      0, batch:   2543, step:    318 - loss: 0.014 (kld=0.001, rec=0.053, per=0.059, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:00 2023 | epoch:      0, batch:   2559, step:    320 - loss: 0.014 (kld=0.001, rec=0.053, per=0.057, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:32:02 2023 | epoch:      0, batch:   2575, step:    322 - loss: 0.020 (kld=0.001, rec=0.073, per=0.082, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:05 2023 | epoch:      0, batch:   2591, step:    324 - loss: 0.018 (kld=0.001, rec=0.045, per=0.096, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:07 2023 | epoch:      0, batch:   2607, step:    326 - loss: 0.018 (kld=0.001, rec=0.070, per=0.075, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:09 2023 | epoch:      0, batch:   2623, step:    328 - loss: 0.015 (kld=0.001, rec=0.050, per=0.070, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:11 2023 | epoch:      0, batch:   2639, step:    330 - loss: 0.019 (kld=0.001, rec=0.069, per=0.084, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:14 2023 | epoch:      0, batch:   2655, step:    332 - loss: 0.014 (kld=0.001, rec=0.047, per=0.067, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:16 2023 | epoch:      0, batch:   2671, step:    334 - loss: 0.019 (kld=0.001, rec=0.075, per=0.079, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:18 2023 | epoch:      0, batch:   2687, step:    336 - loss: 0.013 (kld=0.001, rec=0.039, per=0.063, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:32:21 2023 | epoch:      0, batch:   2703, step:    338 - loss: 0.021 (kld=0.001, rec=0.083, per=0.084, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:23 2023 | epoch:      0, batch:   2719, step:    340 - loss: 0.017 (kld=0.001, rec=0.060, per=0.073, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:25 2023 | epoch:      0, batch:   2735, step:    342 - loss: 0.015 (kld=0.001, rec=0.055, per=0.066, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:27 2023 | epoch:      0, batch:   2751, step:    344 - loss: 0.019 (kld=0.001, rec=0.063, per=0.085, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:30 2023 | epoch:      0, batch:   2767, step:    346 - loss: 0.019 (kld=0.001, rec=0.070, per=0.079, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:32 2023 | epoch:      0, batch:   2783, step:    348 - loss: 0.020 (kld=0.001, rec=0.070, per=0.087, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:34 2023 | epoch:      0, batch:   2799, step:    350 - loss: 0.017 (kld=0.001, rec=0.060, per=0.077, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:36 2023 | epoch:      0, batch:   2815, step:    352 - loss: 0.014 (kld=0.001, rec=0.048, per=0.067, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:32:39 2023 | epoch:      0, batch:   2831, step:    354 - loss: 0.018 (kld=0.001, rec=0.070, per=0.072, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:41 2023 | epoch:      0, batch:   2847, step:    356 - loss: 0.013 (kld=0.001, rec=0.048, per=0.054, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:44 2023 | epoch:      0, batch:   2863, step:    358 - loss: 0.020 (kld=0.001, rec=0.064, per=0.096, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:46 2023 | epoch:      0, batch:   2879, step:    360 - loss: 0.018 (kld=0.001, rec=0.067, per=0.074, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:48 2023 | epoch:      0, batch:   2895, step:    362 - loss: 0.017 (kld=0.001, rec=0.050, per=0.085, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:50 2023 | epoch:      0, batch:   2911, step:    364 - loss: 0.023 (kld=0.001, rec=0.090, per=0.096, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:53 2023 | epoch:      0, batch:   2927, step:    366 - loss: 0.018 (kld=0.001, rec=0.069, per=0.077, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:32:55 2023 | epoch:      0, batch:   2943, step:    368 - loss: 0.021 (kld=0.001, rec=0.088, per=0.083, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:32:57 2023 | epoch:      0, batch:   2959, step:    370 - loss: 0.017 (kld=0.001, rec=0.059, per=0.073, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:00 2023 | epoch:      0, batch:   2975, step:    372 - loss: 0.014 (kld=0.001, rec=0.051, per=0.060, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:02 2023 | epoch:      0, batch:   2991, step:    374 - loss: 0.014 (kld=0.001, rec=0.046, per=0.063, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:04 2023 | epoch:      0, batch:   3007, step:    376 - loss: 0.017 (kld=0.001, rec=0.058, per=0.074, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:06 2023 | epoch:      0, batch:   3023, step:    378 - loss: 0.017 (kld=0.001, rec=0.073, per=0.066, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:09 2023 | epoch:      0, batch:   3039, step:    380 - loss: 0.014 (kld=0.001, rec=0.050, per=0.062, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:11 2023 | epoch:      0, batch:   3055, step:    382 - loss: 0.016 (kld=0.001, rec=0.051, per=0.076, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:13 2023 | epoch:      0, batch:   3071, step:    384 - loss: 0.021 (kld=0.001, rec=0.085, per=0.083, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:33:16 2023 | epoch:      0, batch:   3087, step:    386 - loss: 0.014 (kld=0.001, rec=0.053, per=0.055, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:18 2023 | epoch:      0, batch:   3103, step:    388 - loss: 0.020 (kld=0.001, rec=0.069, per=0.092, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:20 2023 | epoch:      0, batch:   3119, step:    390 - loss: 0.019 (kld=0.001, rec=0.077, per=0.077, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:23 2023 | epoch:      0, batch:   3135, step:    392 - loss: 0.016 (kld=0.001, rec=0.054, per=0.071, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:25 2023 | epoch:      0, batch:   3151, step:    394 - loss: 0.019 (kld=0.001, rec=0.075, per=0.081, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:27 2023 | epoch:      0, batch:   3167, step:    396 - loss: 0.016 (kld=0.001, rec=0.054, per=0.069, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:29 2023 | epoch:      0, batch:   3183, step:    398 - loss: 0.020 (kld=0.001, rec=0.075, per=0.087, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:32 2023 | epoch:      0, batch:   3199, step:    400 - loss: 0.015 (kld=0.001, rec=0.049, per=0.071, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:33:34 2023 | epoch:      0, batch:   3215, step:    402 - loss: 0.017 (kld=0.001, rec=0.058, per=0.081, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:36 2023 | epoch:      0, batch:   3231, step:    404 - loss: 0.013 (kld=0.001, rec=0.042, per=0.060, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:39 2023 | epoch:      0, batch:   3247, step:    406 - loss: 0.012 (kld=0.001, rec=0.034, per=0.057, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:41 2023 | epoch:      0, batch:   3263, step:    408 - loss: 0.014 (kld=0.001, rec=0.056, per=0.058, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:43 2023 | epoch:      0, batch:   3279, step:    410 - loss: 0.021 (kld=0.001, rec=0.078, per=0.087, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:45 2023 | epoch:      0, batch:   3295, step:    412 - loss: 0.010 (kld=0.001, rec=0.032, per=0.050, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:48 2023 | epoch:      0, batch:   3311, step:    414 - loss: 0.019 (kld=0.001, rec=0.067, per=0.084, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:50 2023 | epoch:      0, batch:   3327, step:    416 - loss: 0.022 (kld=0.001, rec=0.081, per=0.094, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:33:53 2023 | epoch:      0, batch:   3343, step:    418 - loss: 0.015 (kld=0.001, rec=0.050, per=0.067, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:55 2023 | epoch:      0, batch:   3359, step:    420 - loss: 0.014 (kld=0.001, rec=0.055, per=0.057, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:57 2023 | epoch:      0, batch:   3375, step:    422 - loss: 0.018 (kld=0.001, rec=0.064, per=0.079, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:33:59 2023 | epoch:      0, batch:   3391, step:    424 - loss: 0.023 (kld=0.001, rec=0.097, per=0.083, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:02 2023 | epoch:      0, batch:   3407, step:    426 - loss: 0.013 (kld=0.001, rec=0.042, per=0.058, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:04 2023 | epoch:      0, batch:   3423, step:    428 - loss: 0.016 (kld=0.001, rec=0.047, per=0.080, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:06 2023 | epoch:      0, batch:   3439, step:    430 - loss: 0.016 (kld=0.001, rec=0.057, per=0.074, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:08 2023 | epoch:      0, batch:   3455, step:    432 - loss: 0.017 (kld=0.001, rec=0.067, per=0.067, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:34:11 2023 | epoch:      0, batch:   3471, step:    434 - loss: 0.020 (kld=0.001, rec=0.087, per=0.073, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:13 2023 | epoch:      0, batch:   3487, step:    436 - loss: 0.015 (kld=0.001, rec=0.059, per=0.063, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:15 2023 | epoch:      0, batch:   3503, step:    438 - loss: 0.015 (kld=0.001, rec=0.052, per=0.070, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:18 2023 | epoch:      0, batch:   3519, step:    440 - loss: 0.014 (kld=0.001, rec=0.051, per=0.063, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:20 2023 | epoch:      0, batch:   3535, step:    442 - loss: 0.024 (kld=0.001, rec=0.085, per=0.105, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:22 2023 | epoch:      0, batch:   3551, step:    444 - loss: 0.015 (kld=0.001, rec=0.054, per=0.063, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:24 2023 | epoch:      0, batch:   3567, step:    446 - loss: 0.017 (kld=0.001, rec=0.056, per=0.075, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:27 2023 | epoch:      0, batch:   3583, step:    448 - loss: 0.014 (kld=0.001, rec=0.049, per=0.062, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:34:29 2023 | epoch:      0, batch:   3599, step:    450 - loss: 0.014 (kld=0.001, rec=0.048, per=0.067, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:31 2023 | epoch:      0, batch:   3615, step:    452 - loss: 0.021 (kld=0.001, rec=0.077, per=0.086, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:34 2023 | epoch:      0, batch:   3631, step:    454 - loss: 0.013 (kld=0.001, rec=0.044, per=0.056, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:36 2023 | epoch:      0, batch:   3647, step:    456 - loss: 0.020 (kld=0.001, rec=0.085, per=0.075, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:38 2023 | epoch:      0, batch:   3663, step:    458 - loss: 0.016 (kld=0.001, rec=0.060, per=0.069, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:41 2023 | epoch:      0, batch:   3679, step:    460 - loss: 0.014 (kld=0.001, rec=0.048, per=0.063, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:43 2023 | epoch:      0, batch:   3695, step:    462 - loss: 0.018 (kld=0.001, rec=0.064, per=0.077, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:45 2023 | epoch:      0, batch:   3711, step:    464 - loss: 0.019 (kld=0.001, rec=0.078, per=0.075, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:34:48 2023 | epoch:      0, batch:   3727, step:    466 - loss: 0.016 (kld=0.001, rec=0.055, per=0.072, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:50 2023 | epoch:      0, batch:   3743, step:    468 - loss: 0.012 (kld=0.001, rec=0.046, per=0.049, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:52 2023 | epoch:      0, batch:   3759, step:    470 - loss: 0.015 (kld=0.001, rec=0.052, per=0.066, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:54 2023 | epoch:      0, batch:   3775, step:    472 - loss: 0.015 (kld=0.001, rec=0.053, per=0.064, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:57 2023 | epoch:      0, batch:   3791, step:    474 - loss: 0.015 (kld=0.001, rec=0.042, per=0.075, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:34:59 2023 | epoch:      0, batch:   3807, step:    476 - loss: 0.014 (kld=0.001, rec=0.048, per=0.063, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:01 2023 | epoch:      0, batch:   3823, step:    478 - loss: 0.018 (kld=0.001, rec=0.062, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:03 2023 | epoch:      0, batch:   3839, step:    480 - loss: 0.015 (kld=0.001, rec=0.054, per=0.066, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:35:06 2023 | epoch:      0, batch:   3855, step:    482 - loss: 0.017 (kld=0.001, rec=0.064, per=0.070, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:08 2023 | epoch:      0, batch:   3871, step:    484 - loss: 0.028 (kld=0.001, rec=0.121, per=0.100, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:10 2023 | epoch:      0, batch:   3887, step:    486 - loss: 0.019 (kld=0.001, rec=0.073, per=0.080, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:13 2023 | epoch:      0, batch:   3903, step:    488 - loss: 0.018 (kld=0.001, rec=0.069, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:15 2023 | epoch:      0, batch:   3919, step:    490 - loss: 0.020 (kld=0.001, rec=0.073, per=0.085, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:17 2023 | epoch:      0, batch:   3935, step:    492 - loss: 0.016 (kld=0.001, rec=0.059, per=0.072, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:19 2023 | epoch:      0, batch:   3951, step:    494 - loss: 0.018 (kld=0.001, rec=0.076, per=0.066, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:22 2023 | epoch:      0, batch:   3967, step:    496 - loss: 0.019 (kld=0.001, rec=0.072, per=0.081, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:35:24 2023 | epoch:      0, batch:   3983, step:    498 - loss: 0.012 (kld=0.001, rec=0.042, per=0.056, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:27 2023 | epoch:      0, batch:   3999, step:    500 - loss: 0.014 (kld=0.001, rec=0.051, per=0.062, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:29 2023 | epoch:      0, batch:   4015, step:    502 - loss: 0.012 (kld=0.001, rec=0.042, per=0.051, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:31 2023 | epoch:      0, batch:   4031, step:    504 - loss: 0.018 (kld=0.001, rec=0.063, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:33 2023 | epoch:      0, batch:   4047, step:    506 - loss: 0.015 (kld=0.001, rec=0.049, per=0.066, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:36 2023 | epoch:      0, batch:   4063, step:    508 - loss: 0.016 (kld=0.001, rec=0.059, per=0.066, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:38 2023 | epoch:      0, batch:   4079, step:    510 - loss: 0.017 (kld=0.001, rec=0.053, per=0.081, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:40 2023 | epoch:      0, batch:   4095, step:    512 - loss: 0.020 (kld=0.001, rec=0.083, per=0.077, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:35:43 2023 | epoch:      0, batch:   4111, step:    514 - loss: 0.013 (kld=0.001, rec=0.046, per=0.057, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:45 2023 | epoch:      0, batch:   4127, step:    516 - loss: 0.015 (kld=0.001, rec=0.053, per=0.066, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:47 2023 | epoch:      0, batch:   4143, step:    518 - loss: 0.013 (kld=0.001, rec=0.041, per=0.064, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:49 2023 | epoch:      0, batch:   4159, step:    520 - loss: 0.012 (kld=0.001, rec=0.035, per=0.059, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:52 2023 | epoch:      0, batch:   4175, step:    522 - loss: 0.015 (kld=0.001, rec=0.051, per=0.070, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:54 2023 | epoch:      0, batch:   4191, step:    524 - loss: 0.020 (kld=0.001, rec=0.071, per=0.092, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:56 2023 | epoch:      0, batch:   4207, step:    526 - loss: 0.012 (kld=0.001, rec=0.039, per=0.058, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:35:59 2023 | epoch:      0, batch:   4223, step:    528 - loss: 0.017 (kld=0.001, rec=0.064, per=0.070, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:36:01 2023 | epoch:      0, batch:   4239, step:    530 - loss: 0.019 (kld=0.001, rec=0.071, per=0.083, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:03 2023 | epoch:      0, batch:   4255, step:    532 - loss: 0.020 (kld=0.001, rec=0.080, per=0.079, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:06 2023 | epoch:      0, batch:   4271, step:    534 - loss: 0.019 (kld=0.001, rec=0.073, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:08 2023 | epoch:      0, batch:   4287, step:    536 - loss: 0.012 (kld=0.001, rec=0.044, per=0.054, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:10 2023 | epoch:      0, batch:   4303, step:    538 - loss: 0.015 (kld=0.001, rec=0.053, per=0.067, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:12 2023 | epoch:      0, batch:   4319, step:    540 - loss: 0.020 (kld=0.001, rec=0.096, per=0.062, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:15 2023 | epoch:      0, batch:   4335, step:    542 - loss: 0.017 (kld=0.001, rec=0.072, per=0.064, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:17 2023 | epoch:      0, batch:   4351, step:    544 - loss: 0.019 (kld=0.001, rec=0.070, per=0.078, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:36:19 2023 | epoch:      0, batch:   4367, step:    546 - loss: 0.020 (kld=0.001, rec=0.080, per=0.079, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:22 2023 | epoch:      0, batch:   4383, step:    548 - loss: 0.015 (kld=0.001, rec=0.060, per=0.061, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:24 2023 | epoch:      0, batch:   4399, step:    550 - loss: 0.013 (kld=0.001, rec=0.048, per=0.055, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:26 2023 | epoch:      0, batch:   4415, step:    552 - loss: 0.019 (kld=0.001, rec=0.067, per=0.081, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:28 2023 | epoch:      0, batch:   4431, step:    554 - loss: 0.013 (kld=0.001, rec=0.049, per=0.050, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:31 2023 | epoch:      0, batch:   4447, step:    556 - loss: 0.019 (kld=0.001, rec=0.077, per=0.077, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:33 2023 | epoch:      0, batch:   4463, step:    558 - loss: 0.015 (kld=0.001, rec=0.059, per=0.061, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:35 2023 | epoch:      0, batch:   4479, step:    560 - loss: 0.015 (kld=0.001, rec=0.057, per=0.063, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Creating checkpoint...\n",
            "Wed Dec 20 13:36:38 2023 | epoch:      0, batch:   4495, step:    562 - loss: 0.016 (kld=0.001, rec=0.063, per=0.062, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n",
            "Wed Dec 20 13:36:40 2023 | epoch:      0, batch:   4511, step:    564 - loss: 0.017 (kld=0.001, rec=0.051, per=0.081, gen=n/a, dis=n/a), p(R|R)=n/a, p(R|F)=n/a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cuda"
      ],
      "metadata": {
        "id": "S6tzlRGMtGhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZbZr2t-2ZlB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
